{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for data science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pandas Dataframes\n",
    "\n",
    "# summary stats\n",
    "\n",
    "# indexing \n",
    "\n",
    "# groubpy\n",
    "\n",
    "# apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro \n",
    "- prediction vs inference\n",
    "- parametric vs non parametric methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **quantitative** variables can use methods such as:\n",
    "- linear regression\n",
    "- k nearest neighbours\n",
    "\n",
    "For **qualitative/categorical** variables need to use methods such as:\n",
    "- logistic regression\n",
    "- decision trees\n",
    "- LDA/QDA\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{1}{n}{\\sum_i (\\hat{y}_i - y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "1 - \\frac{\\text{sum of squared errors}}{\\text{sum of squared squared deviation in }  y} = 1 - \\frac{\\sum_i (\\hat{y}_i - y_i)^2}{\\sum_i (y_i - \\bar{y})^2} = 1 - \\frac{RSS}{TSS}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also look at residual histograms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use training data to produce estimates for $\\hat{\\beta}_0$  and $\\hat{\\beta}_1$ \n",
    "\n",
    "and predict values of $y$ for any $x$ \n",
    "\n",
    "$$ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing least squares (sum of the square errors)\n",
    "\n",
    "$$RSS   = \\sum_i \\,e_i^2= \\sum_i ( y_i - \\hat{y_i} )^2$$\n",
    "\n",
    "coefficients:\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_i (x_i-\\bar{x})^2 }$$\n",
    "\n",
    "$$ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}$$\n",
    "\n",
    "where $\\bar{y} = \\frac{1}{n} \\sum_i y_i$ and  $\\bar{x} = \\frac{1}{n} \\sum_i x_i$ are the sample means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Linear Regression\n",
    "def sk_learn_lin_reg(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    #fit linear model\n",
    "    regression = Lin_Reg()\n",
    "    regression.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    #predict y-values for values in the testing set \n",
    "    predicted_y = regression.predict(Xtest)\n",
    "    \n",
    "    #score predictions\n",
    "    r = regression.score(Xtest, Ytest)\n",
    "    return r, predicted_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## K nearest neighbours\n",
    "def sk_learn_knn(Xtrain, Ytrain, Xtest, Ytest, k):\n",
    "    #fit knn model\n",
    "    neighbours = KNN(n_neighbors=k)\n",
    "    neighbours.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    #predict y-values for values in the testing set \n",
    "    predicted_y = neighbours.predict(Xtest)\n",
    "    \n",
    "    #score predictions\n",
    "    r = neighbours.score(Xtest, Ytest)\n",
    "    return r, predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('./dataset/dataset_1_full.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = \\hat{\\beta}_0+\\hat{\\beta}_1 \\, x_1+ \\hat{\\beta}_2 \\, x_2 + \\ldots + \\hat{\\beta}_p \\, x_p + \\epsilon$$ \n",
    "\n",
    "\n",
    "Estimate the coefficients: \n",
    "$$\n",
    "{\\bf Y} = \\left( \\begin{array}{c}\n",
    "Y_1  \\\\\n",
    "Y_2  \\\\\n",
    "\\vdots \\\\\n",
    "Y_n  \\end{array} \\right)$$\n",
    "\n",
    "$$\n",
    "{\\bf X} = \\left( \\begin{array}{ccccc}\n",
    "1 & X_{1,1} & X_{1,2} & \\ldots & X_{1,p} \\\\\n",
    "1 & X_{2,1} & X_{2,2} & \\ldots & X_{2,p}  \\\\\n",
    "1 & \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "1 & X_{n,1} & X_{n,2} & \\ldots & X_{n,p} \\end{array} \\right)$$\n",
    "\n",
    "\n",
    "$$\n",
    "{\\bf \\beta} = \\left( \\begin{array}{c}\n",
    "\\beta_0  \\\\\n",
    "\\beta_1  \\\\\n",
    "\\vdots  \\\\\n",
    "\\beta_p \\end{array} \\right)$$\n",
    "\n",
    "OLS estimator:\n",
    "\n",
    "$$ \\hat{\\beta} = {\\bf (X^{T}X)^{-1} X^T Y } $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stat model OLS\n",
    "# ADd constant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add polynomial terms to the predictor matrix and use normal OLS\n",
    "## To make predictions transform predictors into the same shape\n",
    "## Select polynomial degree by examining the R2 as a function of degree for the test and train \n",
    "\n",
    "def polynomial_regression_fit(x, y, degrees):\n",
    "    # Create the poly terms for x,x^2 .. \n",
    "    \n",
    "    n= np.size(y)   # data size \n",
    "    x_poly = np.zeros([n, degrees]) # poly degree \n",
    "\n",
    "    for d in range(1, degrees +1):\n",
    "        x_poly[:, d - 1] = np.power(x, d)  # adding terms \n",
    "\n",
    "    Xt=sm.add_constant(x_poly)\n",
    "    model=sm.OLS(y,Xt)\n",
    "    model_results=model.fit()\n",
    "    return model_results, Xt\n",
    "\n",
    "def polynomial_regression_predict(params, degrees, x):\n",
    "    # # Create the poly terms for x,x^2 ..\n",
    "    n = x.shape[0]\n",
    "    x_poly = np.zeros([n, degrees])\n",
    "    for d in range(1, degrees + 1):\n",
    "        x_poly[:, d - 1] = np.power(x, d)\n",
    "    Xt=sm.add_constant(x_poly)\n",
    "   \n",
    "    # Predict y-vals\n",
    "    y_pred = np.dot(params,Xt.T)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data sets with missing values, the values can be imputed from the rest of the dataset:\n",
    "- KNN tends to perform better when the data is not linear and when the shape of the distribution is odd\n",
    "- linear regression performs better than KNN when the data is somewhat linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Impute missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Testing, training sets\n",
    "\n",
    "def split(data, m):\n",
    "    # data is a pandas dataframe\n",
    "    train=data.sample(frac=m)\n",
    "    test=data.drop(train.index)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set Approach\n",
    "\n",
    "- sometimes don't have test set\n",
    "- need a way for selecting model parameters without using the testing set\n",
    "- Randomly divide the data into two parts the training and validation.\n",
    "- Fit the model on the training set and the fitted model is used to predict the response for the testing points. MSE (or $R^2$) is used a validation error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "The power of simulation: \n",
    "\n",
    "   - Select q randomly subsamples of size q out of the whole dataset\n",
    "   - Subsample with replacement. \n",
    "   - For each subsample we estimate the coefficients $\\beta_1$ \n",
    "   - From this  collection of $\\beta$'s we can estimate SE, confidence intervals etc\n",
    "   \n",
    "   \n",
    "Use Adjusted $R^2$ (or AIC/BIC) because more flexible models will result in larger $R^2$ but over-fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation: k-Fold\n",
    "\n",
    "- Split data into folds. Use one for validation the rest for training and repeat k times\n",
    "- test with different values of K\n",
    "- use selection criterion like R2, adjusted R2, AIC/BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation/Shrinkage Methods\n",
    "To reduce overfitting and improve the accuracy on the test set:\n",
    "- subset selection methods - use least squares to fit a linear model that contains a **subset** of the predictors\n",
    "- can fit a model containing all $p$ predictors using a technique that shrinks the coefficient estimates towards zero.\n",
    "- reduce the variance by increasing the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ordinary least square  regression, minimize RSS \n",
    "\n",
    "$$ RSS = \\sum_{i=1}^{n}\\left(y_i  - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 $$\n",
    "\n",
    "\n",
    "###  Ridge regression (L2)\n",
    "\n",
    "Rhe coefficients are the values that minimize\n",
    "\n",
    "$$ \\sum_{i=1}^{n}\\left(y_i  - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2  + \\lambda \\sum_{j=1}^{p} \\beta_k^2 = RSS + \\lambda \\sum_{j=1}^{p} \\beta_k^2 $$\n",
    "\n",
    "where $\\lambda$ must be larger than zero and it is a tunning parameter. $\\lambda$ has to be determined as well during the fitting procedure. \n",
    "\n",
    " The tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates.\n",
    " \n",
    " When $\\lambda=0$ the extra term has no effect so we go back to OLS and when $\\lambda$ is very large all coefficients approach zero. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test error = bias + variance + irreducible error \n",
    "\n",
    "**Regularization reduces variance through increased the bias**  \n",
    "\n",
    "E.g. Fit a cubic polynomial with a few points.  A small variation in the training set will result in a large change of the OLS coefficients since there will be many combinations to get to the training points. However, ridge regression has to keep those coefficients smaller thus restrict the possibility of overfitting and therefore reduces the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regularisation (L1)\n",
    "\n",
    "Ridge regression has a disadvantage. It does not select a subset of predictors - it will shrink the coefficients but it will not reduce them to zero unless $\\lambda \\rightarrow \\infty$. This makes it is harder to interpret the model, especially when the number of predictors is large. \n",
    "\n",
    "Lasso is an alternative to ridge that overcomes this disadvantage\n",
    "\n",
    "$$ \\sum_{i=1}^{n}\\left(y_i  - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2  + \\lambda \\sum_{j=1}^{p} |\\beta_k| = RSS + \\lambda \\sum_{j=1}^{p} |\\beta_k| $$\n",
    "\n",
    "Ridge and lasso are very similar, both trying to constrain the coefficients with an extra term.\n",
    "\n",
    "- Lasso also shrinks the coefficient estimates towards zero. \n",
    "- The l1 penalty forces  some of the coefficients  to be **exactly** equal to zero \n",
    "- The tuning parameter Î» has to be estimated as with ridge with cross-validation.\n",
    "- Lasso  performs variable selection and therefore easier to interpret (sparse models) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Nearest neighbour classifier\n",
    "## Nearest cluster classifeir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The logistic regression model uses a function, called the $\\textit{logistic}$ function, to model $P(Y=1)$:\n",
    "$$ P(Y=1) = \\frac{e^{\\beta_0+\\beta_1 X}}{1+e^{\\beta_0+\\beta_1 X}}.$$\n",
    "As a result the model will predict $P(Y=1)$ with an $S$-shaped curve which is the general shape of the logistic function.  \n",
    "- $\\beta_0$ shifts the curve right or left\n",
    "- $\\beta_1$ controls how steep the S-shaped curve is.  Note: if $\\beta_1$ is positive, then the pedicted $ P(Y=1)$ goes from zero for small values of $X$ to one for large values of $X$ and if $\\beta_1$ is negative, then $ P(Y=1)$ goes from one for small values of $X$ to zero for large values of $X$.\n",
    "\n",
    "The logistic model can be rewritten as:\n",
    "$$ \\ln\\left(\\frac{P(Y=1)}{1-P(Y=1)}\\right) = \\beta_0+\\beta_1 X.$$\n",
    "The value inside the natural log function, $P(Y=1)/(1-P(Y=1))$, is called the $\\textit{odds}$, thus logistic regression is said to model the $\\textit{log-odds}$ with a linear function of the predictors or features, $X$.  This gives us the natural interpretation of the estimates similar to linear regression: a one unit change in $X$ is associated with a $\\beta_1$ change in the log-odds of $Y=1$; or better yet, a one unit change in $X$ is associated with an $e^\\beta_1$ change in the odds that $Y=1$.  \n",
    "\n",
    "Below are four different logistic models with different values for $\\beta_0$ and $\\beta_1$: $\\beta_0 = 0, \\beta_1 = 1$ is in black, $\\beta_0 = 2, \\beta_1 = 1$ is in red, $\\beta_0 = 0, \\beta_1 = 3$ is in blue, and $\\beta_0 = 0, \\beta_1 = -1$ is in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
